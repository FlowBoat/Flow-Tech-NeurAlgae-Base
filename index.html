<!DOCTYPE HTML>
<html>

<head>
    <title>NeurAlgae</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/3.11.4/math.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/synaptic/1.0.10/synaptic.js"></script>
    <script type="text/javascript">
        function initNeurAlgae() {
            //retrieving data from .jsons
            var request = new XMLHttpRequest();
            request.open("GET", "../data/data.json", false);
            request.send(null)
            var data1 = JSON.parse(request.responseText);
            alert(my_JSON_object.result[0]);

            //Preparing Synaptic
            var Neuron = synaptic.Neuron,
                Layer = synaptic.Layer,
                Network = synaptic.Network,
                Trainer = synaptic.Trainer,
                Architect = synaptic.Architect;

            //For Pseudo Nitzschia quantities
            var NeurAlgaePN = new Perceptron(10, 30, 3);
            var trainingSetPN = data1;
            var trainingOptions = {
                rate: .1,
                iterations: 20000,
                error: .005,
                cost: Trainer.cost.CROSS_ENTROPY
            }

            NeurAlgae.trainer.trainAsync(trainingSet, trainingOptions)
                .then(results => console.log('done!', results));

            var standalone = NeurAlgae.standalone();
        }

        //Get Dom elelement Stuff: inMatrix formated [ , , , , , , , , , , ]
        function PNrequest() {
            //input = JSON.parse(document.getElementsByName("inMatrix1")[0].value);
            document.getElementsByName("outputBox1")[0].value = String(Math.random());
        }

        function PNrequest2() {
            //input = JSON.parse(document.getElementsByName("inMatrix1")[0].value);
            document.getElementsByName("outputBox2")[0].value = String(Math.random());
        }
        //var output = standalonePN(input);


        //function LSTM(input, blocks, output) {
        //    //create the layers
        //    var inputLayer = new Layer(input);
        //    var inputGate = new Layer(blocks);
        //    var forgetGate = new Layer(blocks);
        //    var memoryCell = new Layer(blocks);
        //    var outputGate = new Layer(blocks);
        //    var outputLayer = new Layer(output);
        //
        //    //Conncections from input layer
        //    var input = inputLayer.project(memoryCell);
        //    inputLayer.project(inputGate);
        //    inputLayer.project(forgetGate);
        //    inputLayer.project(outputGate);
        //
        //    //conncections from memoryCell
        //    var output = memoryCell.project(outputLayer);
        //}
        //
        ////self-connection
        //var self = memoryCell.project(memoryCell);
        //
        ////peepholes
        //memoryCell.project(inputGate);
        //memoryCell.project(forgetGate);
        //memoryCell.project(outputGate);
        //
        ////gates
        //inputGate.gate(input, Layer.gateTpe.INPUT);
        //forgetGate.gate(self, layer.gateType.ONE_TO_ONE);
        //outputGate.gate(output, Layer.gateType.OUTPUT);
        //
        ////input to output direct connection
        //inputLayer.project(outputLayer);
        //this.set({
        //    input: inputLayer,
        //    hidden: [inputGate, forgetGate, memoryCell, outputGate],
        //    output: outputLayer;
        //})
        //
        ////extend prototype chain
        //LSTM.prototype = new Network();
        //LSTM.prototype.constructor = LSTM;
    </script>
    <!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body>

    <!-- Wrapper -->
    <div id="wrapper">

        <!-- Header -->
        <header id="header">
            <div class="logo">
                <span class="icon fa-tint"></span>
            </div>
            <div class="content">
                <div class="inner">
                    <h1 style="font-family: 'Lato', sans-serif; text-transform: none;">NeurAlgae</h1>
                    <p>
                        <!--[-->A Novel Approach to Harmful Algal Bloom Prediction Using Machine Learning
                        <!--]--><br />
                        <!--[-->By Atif Mahmud and Zachary Trefler
                        <!--]-->
                    </p>
                </div>
            </div>
            <nav>
                <ul>
                    <li><a href="#intro">Intro</a></li>
                    <li><a href="#vision">Vision</a></li>
                    <li><a href="#analysis">Analysis</a></li>
                    <!--<li><a href="#app">App</a></li>-->
                    <li><a href="#code">Code</a></li>
                    <li><a href="#contact">About</a></li>
                    <!--<li><a href="#elements">Elements</a></li>-->
                </ul>
            </nav>

        </header>

        <!-- Main -->
        <div id="main">

            <!-- Intro -->
            <article id="intro">
                <h2 class="major">Background & Introduction</h2>
                <span class="image main"><img src="images/pic01.jpg" alt="" /></span>
                <p>Algal blooms, the sudden and rapid reproduction of microscopic algae or cyanobacteria in water bodies, have recently become a serious ecological issue threatening both the vitality of their local ecosystems and the socioeconomic conditions
                    of their affected areas. Many of these algal blooms are caused by eutrophication (excesses of nutrients in the water), often due to local farm fertilizer runoff, have the potential to engulf entire coastal regions. These enormous blooms
                    deplete sunlight in the area, as well as absorbing large quantities of dissolved oxygen when they decay, depriving the local aquatic ecosystems of vital resources and causing them to collapse into dead zones. In addition, the algae
                    within these blooms often produce toxic chemicals as by-products of their metabolism. </p>
                <p>These algal blooms, known as Harmful Algal Blooms or HABs, are becoming increasingly prevalent in today's society with the advent of modern agricultural techniques without sufficiently sophisticated environmental precautions to match.
                    If the presence of algal blooms could be predicted, recovery efforts could be optimized to coincide with the algal blooms' initial outbreak, minimizing both the ecological and socioeconomic costs of the blooming. Even as the world
                    of real-time oceanographic monitoring systems becomes more accessible and descriptive, early response efforts are still limited by the scope of the knowledge that these systems provide them; specifically, by the fact that the information
                    that these systems provide is real-time at best and at worst, abandoned. Therefore a more elegant and efficient solution must be proposed.</p>
                <p>Machine learning is the very field of computer science that is developed enough to provide such a solution. This field teaches computers to “learn” by subjecting them to a matrix of input data, encouraging it to make correlations between
                    those inputs and predetermined outputs. By doing so, computers are able to understand, manipulate and create relationships between data that would otherwise be incomprehensible to humans. In this way, a computer could identify the
                    direct relationship between the environmental factors affecting a body of water and the possibility of a harmful algal bloom occurring there.</p>
                <span class="image main"><img src="images/pic04.jpg" alt="" /></span>
                <p>The National Oceanic and Atmospheric Administration’s (NOAA’s) OPENDAP database contains a plethora of location tagged oceanographic data freely accessible through there ERDDAP server. When combined with NASA and CENCOOS’ algal bloom realtime
                    HAB probability database, one would easily be able to retrieve and replicate the findings of this project.</p>
            </article>

            <!-- Design -->
            <article id="vision">
                <h2 class="major">The Vision</h2>
                <span class="image main"><img src="images/pic02.jpg" alt="" /></span>
                <h3>Abstract</h3>
                <p>Algal blooms, the sudden reproduction of microscopic algae or cyanobacteria in water bodies, have recently become a serious ecological issue. These algal blooms, caused by eutrophican, often due to fertilizer runoff, can deplete local
                    sunlight, absorb dissolved oxygen, and produce toxic chemicals. Because of modern agricultural techniques, HABs are rapidly becoming a large ecological and economic issue. To counter this problem, we propose a method of implementing
                    machine learning to predict the occurence of HABs using oceanographic data. We used an Artificial Neural Network (ANN) to correlate water measurements and the probabilities of finding three HAB indicators (Pseudo-Nitzschia cells, cellular
                    domoic acid, and particulate domoic acid) above certain threshold values, producing an accurate predictive model.</p>
                <h3>Purpose</h3>
                <p>The purpose of this project is to develop an Artificial Neural Network which can effectively predict the probability of a Harmful Algal Bloom event occurring in an arbitrary location, given certain data about the water in the region. By
                    training the network, a model will be produced with the network's final sets of weights and biases of the network, which can be used to quickly forward any input data to receive ouptput data.</p>
                <h3>Design Criteria</h3>
                <p>The program mus be able to retrieve sequential marine environment data from a NOAA OPENDAP database, update and train itself in realtime, accurately deterimine the probability of: harmful abundances of high toxicity, domoic acid carrying
                    algae species. Also the web app should facilitate the easy access to the NeurAlgae prediction algorithm for the lay person, in order to increase the response time of municipal utility agencies.</p>
                <h3>Design Method and Challenges</h3>
                <p>The primary difficulty in implementing an ANN-based solution to this problem was not the creation of any network by itself, but rather the decision of how to design the network in order to optimize its effectiveness without encountering
                    any of the many difficulties associated with machine-learning optimization problems. After experimenting with many different possible ANN varieties, including various activation functions, cost functions, optimization methods, regularization
                    methods, etc. the final design of the ANN made use of entirely sigmoid neurons, the cross-entropy cost function, stochastic gradient descent optimization using back-propagation, and L2 regularization. Another difficulty encountered
                    was the scarcity of complete data to train the network with, which was fully supervised.</p>
                <span class="image main"><img src="images/pic17.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic07.jpg" alt="" /></span>
                <h3>Use of Sequential Data Analysis to Improve Generalization</h3>
                <p>An important distinction between this approach and other possible models is the consideration of adjacent data points in the training of the next in a sequential dataset. By examining the probability of neighboring data points in a linear
                    sequential set, the computer is able to retrieve and learn from the probability of an algal bloom from the last timestamp. This results in the network behaving recurrently; that is, the network is able to pass its outputs into its
                    next input matrix, allowing a limited capacity for simulated memory.</p>
                <span class="image main"><img src="images/pic05.jpg" alt=""></span>
                <span class="image main"><img src="images/pic06.jpg" alt=""></span>
                <p>Using this pseudo-recurrence in a deep neural network, the computer is able to make far more accurate predictions. If the lay user inputs a sequential data series for a given monitoring station, the neural network is able to use historical
                    forecast data to increase its accuracy and make more precise predictions for the given data series. As shown in the figure above the number of false positives decreases overtime as the network is able to learn from its own behaviour
                    and forecast accuracy.</p>
            </article>

            <!-- Analysis -->
            <article id="analysis">
                <h2 class="major">Analysis</h2>
                <!--<span class="image main"><img src="images/pic03.jpg" alt="" /></span>-->
                <h3>Results and analysis</h3>
                <p>Our neural network model was used to build four separate networks, nn30, nn30r, nn5050, and nn5050r. The networks marked '30' had 30 hidden nodes in one hidden layer, and the '5050' networks had 100 hidden nodes evenly in two hidden layers.
                    The networks without the letter 'r' took our 10 pieces of input data, while the 'r' networks took that and three pieces of past output data, allowing for pseudo-recurrence. The networks were created and trained using two thirds of
                    our dataset, and tested using the remaining third. We used validation techniques to choose the same learning rate and regularization parameters for each network. All four networks trained differently, but they all trained well without
                    overfitting. Each network's error values (the average differences between the network outputs and the output data) went down by orders of magnitude before the cost function minimized.</p>
                <hr>
                </span><a href="#appendicies" class="button special">Visit Appendicies</a>
                <hr>
                <p>When given input data, the trained networks gave output values which were not exactly the same as the given algal bloom probabilities, but were in a close range - which is the optimal result for this problem, since the empirical data can
                    vary so much, a fairly wide margin to account for differences is necessary in the model to prevent overfitting, and yet the model still is able to produce output data that is accurate enough to give a good idea of the algal bloom chances
                    in that region.</p>
                <h3>Conclusion</h3>
                <p>Our program meets its design criteria by accurately and efficiently predicting the probability of HAB events to within a reasonable error margin. While, of course, the fit of the network is somewhat imprecise, to fit much more would result
                    in overfitting of the empirical and somewhat noisy data. Meanwhile, the output values given are close enough to expected output values that they could be used for the prediction of HABs - not in precise quantities, which is nearly
                    impossible given the complexity of ocean ecosystems, but with enough precision to accurately determine data quickly for a given location, give an estimate of its severity, and allow humans to respond accordingly. Since our prediction
                    takes in 10 data values, it is already an improvement over human methods, since human brains cannot keep track of complex 10-parameter models in their heads.</p>
                <h3>Next steps</h3>
                <p>While the progam could not train further using the methods outlined here, there are ways to improve its performance. Most oceanographic data does not measure a complete set of data for our model, and historical data is often limited, so
                    our datasets were on the small side. Much more data would result in a more accurately trained network. Additionally, one of the largest improvements our model could use would be to transition to fully-recurrent networks. These networks
                    are more complicated and would allow our model to better compute the correlations. Finally, our model currently uses inputs of oceanographic data, but if satellite data could be given as a secondary input source, and the network given
                    a classification task, our accuracy could be improved, perhaps even to rival the C-HARM model we used to obtain our datasets (although this would sacrifice some computational efficiency).
                </p>
                <h3>Acknowledgements</h3>
                <p>Thank you to our parents for supporting our project throughout its development, and constantly allowing us the time to work on it. Thanks also to Mr. Menhennet, who provided guidance throughout the project on both formatting issues and
                    geographical data.</p>
                <h3 id="appendicies">Appendicies</h3>
                <span class="image main"><img src="images/pic08.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic10.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic11.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic12.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic13.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic14.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic15.jpg" alt="" /></span>
                <span class="image main"><img src="images/pic16.jpg" alt="" /></span>
            </article>

            <!-- Contact -->
            <article id="contact">
                <h2 class="major">About Us</h2>
                <span class="image left"><img src="images/us.jpg" alt=""></span> Hi, in all the rush of Neural Networks and their application in Algal Bloom prediction, we forgot to mention to you who we were! We're Atif and Zach, partners, long-time
                pals and best friends. We've been in the same class for 7 years and counting and it's been a blast! Our friendship goes way back and so does our love of science. This is our second year doing a joint project and it has been the most fun
                of all our years of science fair. If you have any questions, comments or concerns feel free to hit us up at any of our socials!
                <hr>
                <h4>Atif Mahmud</h4>
                <p>Personal Email: atifmahmud101@gmail.com <br>School Email: mahma6337@googleapps.wrdsb.ca <br>Cell: (226) 606-9535</p>
                <ul class="icons">
                    <li><a href="#https://twitter.com/Atif_Mahmud101" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
                    <li><a href="#https://www.facebook.com/Atif.A.Mahmud" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
                    <li><a href="#https://www.instagram.com/Atif.A.Mahmud/" class="icon fa-instagram"><span class="label">Instagram</span></a></li>
                    <li><a href="#https://github.com/Atif-Mahmud" class="icon fa-github"><span class="label">GitHub</span></a></li>
                </ul>
                <h4>Zach Trefler</h4>
                <p>Personal Email: zmct99@gmail.com <br>School Email: trefz7495@googleapps.wrdsb.ca <br>Cell: (226) 972-0492</p>
                <ul class="icons">
                    <li><a href="#https://www.facebook.com/zaki.tref" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
                    <li><a href="#https://github.com/zmct" class="icon fa-github"><span class="label">GitHub</span></a></li>
                </ul>
            </article>

            <article id="app">
                <h2 class="major">HAB Prediction</h2>
                <h3>Before you start!</h3>
                <p>Click the button below to instance a unique network (async)</p>
                <a onclick="initNeurAlgae()" class="button">Train</a>
                <hr>
                <h3>For a single data point:</h3>
                <p>Format your matrix as follows:</p>
                <pre><code>SL = //Sea surface height above sea level
WP = //Sea water pressure
CH = //Mass concentration of chlorophyll-A in water
WT = //Sea water temperature
EC = //Sea water electrical conductivity
O2 = //Mass concentration of dissolved oxygen
NO3 = //Molar concentration of nitrate in water
fO2 = //Fractional concentration of oxygen in water
PS = //Sea water practical salinity
TU = //Water turbidity
[SL, WP, CH, WT, EC, O2, NO3, fO2, PS, TU]</code></pre>
                <form action="">
                    <div class="field">
                        <label for="inMatrix1">Input Parameter Matrix</label>
                        <textarea name="inMatrix1" id="inMatrix1" placeholder="Enter your input matrix here" rows="1"></textarea>
                    </div>
                </form>
                <h3>Select output type</h3>
                <ul class="actions" style="width: %100">
                    <div style="text-align: center">
                        <li><a onclick="PNrequest()" class="button special">Pseudo Nitzschia</a></li>
                        <li><a onclick="PNrequest()" class="button special">Domic acid</a></li>
                        <li><a onclick="PNrequest()" class="button special">Pt. Domoic Acid</a></li>
                    </div>
                </ul>
                <textarea readonly name="outputBox1" id="outputBox1" placeholder="" cols="16em" rows="1"></textarea>

                <hr>
                <h3>For multiple data points (sequential):</h3>
                <p>Format your matrix as follows:</p>
                <pre><code>SL = //Sea surface height above sea level
WP = //Sea water pressure
CH = //Mass concentration of chlorophyll-A in water
WT = //Sea water temperature
EC = //Sea water electrical conductivity
O2 = //Mass concentration of dissolved oxygen
NO3 = //Molar concentration of nitrate in water
fO2 = //Fractional concentration of oxygen in water
PS = //Sea water practical salinity
TU = //Water turbidity
[SL, WP, CH, WT, EC, O2, NO3, fO2, PS, TU]</code></pre>
                <form action="">
                    <div class="field">
                        <textarea name="numb" id="numb" placeholder="Enter the number of data points" rows="1"></textarea>
                        <br>
                        <label for="inMatrix2">Input Sequential Parameter Matrix</label>
                        <textarea name="inMatrix2" id="inMatrix2" placeholder="Enter your sequential input matrix here" rows="6"></textarea>
                    </div>
                </form>
                <h3>Select output type</h3>
                <ul class="actions" style="width: %100">
                    <div style="text-align: center">
                        <li><a onclick="PNrequest2()" class="button special">Pseudo Nitzschia</a></li>
                        <li><a onclick="PNrequest2()" class="button special">Domic acid</a></li>
                        <li><a onclick="PNrequest2()" class="button special">Pt. Domoic Acid</a></li>
                    </div>

                </ul>
                <textarea readonly name="outputBox2" id="outputBox2" placeholder="" cols="16em" rows="1"></textarea>
                <hr>
            </article>

            <!--
                <
                script >
                    var weights = math.matrix([
                        [
                            [0.402671508221699, 0.3145443819411189, -0.31683992006811007, -0.1517665581035305, -0.21090324140209352, 0.5122068165991946, -0.0006511662645550958, 0.37426109387739015, 0.06106824444194699, -0.503116407515812],
                            [0.08136751229746456, -0.22131209969032054, 0.0015012624975106888, 0.08012292871082094, 0.1289502518694768, 0.29305811496176876, 0.33708870438559, -0.11008712445764685, -0.14608006861811196, 0.10028344808589244],
                            [0.11539306367373382, 0.18165984445563688, 0.009230702588803229, 0.19595525833851002, 0.059810275664852204, 0.2850591952484085, -0.27625162784545654, 0.005654771638098331, -0.11434079860011576, -0.028439377259216327],
                            [0.0402629653747654, -0.33985461312437143, 0.48985535648056633, -0.351310508033789, -0.25036860429531743, 0.1713230084420036, 0.06146000208759397, 0.026674395701371523, 0.06760020233693366, 0.026622147682951756],
                            [-0.29885989932532064, 0.326215865652264, 0.5015494337491093, 0.28527325886075877, 0.16464688843221809, -0.14739634484794803, -0.24502051409705528, -0.5400219392271166, 0.1952759222474423, -0.4633105113389218],
                            [-0.0714859774626854, 0.11962190389215767, 0.15923910968781851, -0.20805428181535765, -0.800485696385979, -0.26065070814752256, -0.0011988572057589242, 0.13380037536722275, -0.0927333357780101, 0.19943565908897593],
                            [0.13962683597584344, 0.35741205191123226, -0.08392373623434606, -0.4592084668487909, 0.10504133361626579, -0.1482881180761667, -0.22458248340816053, 0.12635385732540075, -0.23930126141390756, 0.3802809560857692],
                            [-0.1732666516100773, -0.20361639214669636, 0.21544902906223548, 0.07626393259433072, -0.08258299584031799, 0.41517234993763596, -0.06818055883615715, -0.40246758388426446, 0.20407946374956773, -0.4343974646610449],
                            [0.02084641407273736, 0.12148568573467293, -0.24481113097827142, -0.6092692967487343, 0.38939401487647446, -0.4821200757115209, -0.06357128089092515, -0.44719780732759107, 0.5330774447210006, 0.44699051802844386],
                            [-0.023207490220516807, 0.49614760051914836, 0.18372711699588554, 0.035669804725451365, -0.3143243050307672, -0.15362956645619294, 0.2979617324120433, -0.2194102697566899, -0.06937809553438547, -0.0896123489168049],
                            [-0.017444256054382875, 0.03898229300187158, -0.5284875105838042, -0.21883859887901186, 0.5920833195017373, 0.2267357737642589, -0.5172081280746372, 0.23870219643895377, 0.1298739609966815, 0.622558380671965],
                            [0.3547773647216016, -0.1021636917370754, 0.05148261839166087, 0.028589704449831624, -0.3079950801328334, -0.07892359972547207, 0.11694332031386563, -0.4696408118419394, 0.13387043381703037, -0.33292944448509854],
                            [-0.011822396829808464, 0.216645264164245, 0.1975798271747695, -0.35726069396490584, 0.1633668411247472, -0.27219887995501446, -0.5830235796512514, 0.06745423018263177, -0.13633505204794924, -0.27561651865263853],
                            [-0.18445235002327828, -0.16611527222521752, 0.3240680746983588, 0.05975110903369772, 0.386371481124386, -0.1187504276447001, -0.5510141690804339, 0.2568887053537992, -0.3176512692379286, -0.03451913300517021],
                            [0.3441710769284634, 0.013159910576428136, -0.2523527863114449, 0.534114635749501, 0.0021511337933465395, -0.8601941737027833, 0.4145674110441317, -0.044004391116243574, 0.29338617022787516, -0.35500655959676675],
                            [0.550206064222658, 0.03589768544121508, -0.45916272755502624, 0.41864158900338977, 0.2578872080596533, -0.40523389404713567, -0.10286850582401226, 0.20359926862323394, 0.12344986504490162, -0.8419018846221341],
                            [0.2912527049243826, -0.5524202264378046, -0.2423096256002224, -0.1012235302858454, 0.5964001222203992, 0.017768134633082744, 0.6667274997430078, -0.5427650568250869, -0.3290598871731041, -0.27252025124218],
                            [-0.08254957751816029, -0.35092592557122526, -0.045743579954857616, -0.37745040818240816, 0.42088634276912734, -0.3843033197605242, 0.5427468512966296, 0.173809858652688, -0.058652413814565596, -0.4298715350737313],
                            [-0.08181745507850131, 0.32966689156837037, 0.11646886708598085, -0.0847426966285749, 0.023829120602181796, -0.46076371519898707, -0.31733678623000294, 0.02006296877375756, 0.45004492957216347, -0.3504131997092668],
                            [-0.44006825385938464, -0.15629018268814746, -0.2795372406100484, 0.34637597420306193, 0.18166893715341584, 0.35613262782816985, 0.5504514479776165, 0.19499787152706483, 0.1283796673532504, 0.40629199992965026],
                            [-0.5682518040345811, -0.2824094513351995, -0.03662947927593581, -0.3640636539772349, 0.12421432889360978, 0.057229700164703676, 0.1751985083283624, -0.4649821891423733, 0.11535928630225419, -0.01294528103465536],
                            [-0.24786831233906328, -0.37884183444395647, -0.3581070671527153, -0.2789755784712483, -0.08479506332795332, -0.20506114233561148, -0.0010852099704944287, 0.4238786634238386, 0.3538826769370612, 0.658522319607397],
                            [-0.2386531663960104, 0.35396812553925316, -0.5096729168156166, -0.1010910277174483, -0.1789932295564152, -0.15182077716020137, 0.25062331798630016, 0.227779868581985, -0.29461070224482205, 0.12911703716380712],
                            [0.17137802065642835, 0.08157678240509116, -0.09298895077703563, -0.2628408989738799, -0.4085790950639851, -0.5160163073785606, -0.1265422545249111, 0.3779590725878531, -0.1846732913593282, 0.4103054680620023],
                            [0.10274041330701152, -0.12331777697259685, 0.3624850868214905, -0.02422794071033323, -0.499754231438732, -0.2326078763311384, -0.045664112769943366, 0.0005386706719722756, -0.06859404047647541, -0.6824242063945997],
                            [-0.3044014688048265, 0.044112185945771314, 0.15207277764807609, 0.604221891849348, -0.005740227943873254, 0.11739330139735388, -0.2662113578790332, 0.054162694123955836, -0.1758638011283301, 0.22252339640652616],
                            [-0.09869592145969949, -0.27352530937415254, 0.48978424883631233, 0.33835413613835913, -0.38289423572970976, -0.4343411815910688, 0.4197221603308053, -0.32338268741987364, 0.11199912031319233, -0.2604923248848677],
                            [0.6294708697855617, 0.0445251605549205, -0.496030618088207, -0.5801466456328007, -0.3395129599314435, -0.3234664857532911, 0.16750423944339854, -0.7357802799842111, -0.4172057004030097, -0.03762938641814439],
                            [-0.17281817719455378, -0.436221988351252, -0.13933266763666988, -0.21417661008552724, 0.2734049747740402, 0.42455841542748907, 0.895376555748988, 0.2089149145268151, 0.2982801282585924, -0.34337220020344006],
                            [0.23342130283380924, 0.5361982224245471, 0.10993113894872833, 0.058696296016159666, 0.3316860977826941, -0.15012197097664925, -0.3686289311607766, -0.1748925982165964, -0.08878637791136315, 0.46314363134558284]
                        ],
                        [
                            [0.35908364396985953, 0.17609093440787676, -0.1387282975797684, 0.3624130632933994, 0.11088595165251515, -0.025297747430310127, 0.2568969958795377, -0.07664349304623214, -0.11032889594155149, -0.002840083490391532, 0.09037256630661077, 0.08874805481716105, 0.08129763113199255, 0.05744922229678959, 0.2726179343111334, 0.01508016568390385, -0.09447416998799946, -0.06108316906046407, 0.18472672599369078, 0.15589634157109278, 0.031133932791967474, 0.09056872602651166, 0.14365992372154812, -0.2469322332437878, 0.4431100869356929, 0.43740416466679954, 0.018910779248729265, 0.12571714202593426, -0.16520059546708257, 0.1660779111161738],
                            [-0.10915618005802531, 0.0013191448771558566, 0.0974039486323895, -0.5928962618934296, -0.11362826872900066, -0.25969287594383367, -0.11076426266174044, -0.1610919912762249, -0.1393217249832279, -0.1645577327206121, 0.08838440884973864, 0.22039554399464106, -0.1054372552103032, 0.024509201050635664, -0.026887133247703934, -0.328596844753325, -0.2111370451633122, -0.2392423898683764, -0.09949192652709433, -0.06338452646805395, -0.25147011024475113, -0.16999624481139403, -0.19638837916422688, 0.17465462435814313, -0.4351567091467434, -0.20012391583438732, 0.04446295652069178, -0.15466253148128517, -0.020892094404367182, -0.12910888677992924],
                            [0.22940889063650324, -0.023110011984972915, -0.1650992202388904, 0.18407623693003516, 0.09538244516087707, -0.08623439351850573, 0.2667456438942805, -0.024398270728176994, -0.09541091336449652, -0.35014903123456886, 0.2201429582506896, -0.11815701928877223, -0.05585617193486395, 0.02143012946888866, 0.09563117240754386, -0.4427725163302287, 0.16840350683933616, -0.08885536752399253, 0.00703885330192659, -0.20301523477101224, 0.053445147464552534, -0.2648014259912377, -0.19036472918172317, -0.15882748327167823, 0.0643257551646384, -0.0721758970930173, -0.04695777258126396, 0.11170935243747887, 0.033834822021532894, -0.02680323457997105]
                        ]
                    ]);

                var bias = math.matrix([
                    [
                        [1.06348228511347],
                        [-0.11327202867606384],
                        [-1.031739213375915],
                        [1.2694728565047546],
                        [-0.6703697192549465],
                        [0.4745247124688098],
                        [1.3147236161446545],
                        [-0.24673802474345344],
                        [1.2691743159560047],
                        [0.2617534868593826],
                        [-0.1547318773291093],
                        [-0.5561832711897426],
                        [1.617617947665187],
                        [-1.3592012028512779],
                        [-0.10245601992474457],
                        [0.48475459884445327],
                        [-0.0922594824651141],
                        [-0.3512996118795069],
                        [-0.4805655821797293],
                        [0.6553924294867092],
                        [-1.3030067231329585],
                        [-0.05763970731179326],
                        [-1.3027530082305838],
                        [-1.902305796401518],
                        [0.8881386397991767],
                        [1.7347206375856816],
                        [-1.0431479894941613],
                        [1.0030086773973337],
                        [0.3678379994761921],
                        [-1.2108344932438393]
                    ],
                    [
                        [0.38906917498260357],
                        [-0.17306228936431656],
                        [0.21197438957071563]
                    ]
                ]);

                var zipped = math.matrix([
                    [
                        [0.402671508221699,
                            0.3145443819411189, -0.31683992006811007, -0.1517665581035305, -0.21090324140209352,
                            0.5122068165991946, -0.0006511662645550958,
                            0.37426109387739015,
                            0.06106824444194699, -0.503116407515812
                        ],
                        [0.08136751229746456, -0.22131209969032054,
                            0.0015012624975106888,
                            0.08012292871082094,
                            0.1289502518694768,
                            0.29305811496176876,
                            0.33708870438559, -0.11008712445764685, -0.14608006861811196,
                            0.10028344808589244
                        ],
                        [0.11539306367373382,
                            0.18165984445563688,
                            0.009230702588803229,
                            0.19595525833851002,
                            0.059810275664852204,
                            0.2850591952484085, -0.27625162784545654,
                            0.005654771638098331, -0.11434079860011576, -0.028439377259216327
                        ],
                        [0.0402629653747654, -0.33985461312437143,
                            0.48985535648056633, -0.351310508033789, -0.25036860429531743,
                            0.1713230084420036,
                            0.06146000208759397,
                            0.026674395701371523,
                            0.06760020233693366,
                            0.026622147682951756
                        ],
                        [-0.29885989932532064,
                            0.326215865652264,
                            0.5015494337491093,
                            0.28527325886075877,
                            0.16464688843221809, -0.14739634484794803, -0.24502051409705528, -0.5400219392271166,
                            0.1952759222474423, -0.4633105113389218
                        ],
                        [-0.0714859774626854,
                            0.11962190389215767,
                            0.15923910968781851, -0.20805428181535765, -0.800485696385979, -0.26065070814752256, -0.0011988572057589242,
                            0.13380037536722275, -0.0927333357780101,
                            0.19943565908897593
                        ],
                        [0.13962683597584344,
                            0.35741205191123226, -0.08392373623434606, -0.4592084668487909,
                            0.10504133361626579, -0.1482881180761667, -0.22458248340816053,
                            0.12635385732540075, -0.23930126141390756,
                            0.3802809560857692
                        ],
                        [-0.1732666516100773, -0.20361639214669636,
                            0.21544902906223548,
                            0.07626393259433072, -0.08258299584031799,
                            0.41517234993763596, -0.06818055883615715, -0.40246758388426446,
                            0.20407946374956773, -0.4343974646610449
                        ],
                        [0.02084641407273736,
                            0.12148568573467293, -0.24481113097827142, -0.6092692967487343,
                            0.38939401487647446, -0.4821200757115209, -0.06357128089092515, -0.44719780732759107,
                            0.5330774447210006,
                            0.44699051802844386
                        ],
                        [-0.023207490220516807,
                            0.49614760051914836,
                            0.18372711699588554,
                            0.035669804725451365, -0.3143243050307672, -0.15362956645619294,
                            0.2979617324120433, -0.2194102697566899, -0.06937809553438547, -0.0896123489168049
                        ],
                        [-0.017444256054382875,
                            0.03898229300187158, -0.5284875105838042, -0.21883859887901186,
                            0.5920833195017373,
                            0.2267357737642589, -0.5172081280746372,
                            0.23870219643895377,
                            0.1298739609966815,
                            0.622558380671965
                        ],
                        [0.3547773647216016, -0.1021636917370754,
                            0.05148261839166087,
                            0.028589704449831624, -0.3079950801328334, -0.07892359972547207,
                            0.11694332031386563, -0.4696408118419394,
                            0.13387043381703037, -0.33292944448509854
                        ],
                        [-0.011822396829808464,
                            0.216645264164245,
                            0.1975798271747695, -0.35726069396490584,
                            0.1633668411247472, -0.27219887995501446, -0.5830235796512514,
                            0.06745423018263177, -0.13633505204794924, -0.27561651865263853
                        ],
                        [-0.18445235002327828, -0.16611527222521752,
                            0.3240680746983588,
                            0.05975110903369772,
                            0.386371481124386, -0.1187504276447001, -0.5510141690804339,
                            0.2568887053537992, -0.3176512692379286, -0.03451913300517021
                        ],
                        [0.3441710769284634,
                            0.013159910576428136, -0.2523527863114449,
                            0.534114635749501,
                            0.0021511337933465395, -0.8601941737027833,
                            0.4145674110441317, -0.044004391116243574,
                            0.29338617022787516, -0.35500655959676675
                        ],
                        [0.550206064222658,
                            0.03589768544121508, -0.45916272755502624,
                            0.41864158900338977,
                            0.2578872080596533, -0.40523389404713567, -0.10286850582401226,
                            0.20359926862323394,
                            0.12344986504490162, -0.8419018846221341
                        ],
                        [0.2912527049243826, -0.5524202264378046, -0.2423096256002224, -0.1012235302858454,
                            0.5964001222203992,
                            0.017768134633082744,
                            0.6667274997430078, -0.5427650568250869, -0.3290598871731041, -0.27252025124218
                        ],
                        [-0.08254957751816029, -0.35092592557122526, -0.045743579954857616, -0.37745040818240816,
                            0.42088634276912734, -0.3843033197605242,
                            0.5427468512966296,
                            0.173809858652688, -0.058652413814565596, -0.4298715350737313
                        ],
                        [-0.08181745507850131,
                            0.32966689156837037,
                            0.11646886708598085, -0.0847426966285749,
                            0.023829120602181796, -0.46076371519898707, -0.31733678623000294,
                            0.02006296877375756,
                            0.45004492957216347, -0.3504131997092668
                        ],
                        [-0.44006825385938464, -0.15629018268814746, -0.2795372406100484,
                            0.34637597420306193,
                            0.18166893715341584,
                            0.35613262782816985,
                            0.5504514479776165,
                            0.19499787152706483,
                            0.1283796673532504,
                            0.40629199992965026
                        ],
                        [-0.5682518040345811, -0.2824094513351995, -0.03662947927593581, -0.3640636539772349,
                            0.12421432889360978,
                            0.057229700164703676,
                            0.1751985083283624, -0.4649821891423733,
                            0.11535928630225419, -0.01294528103465536
                        ],
                        [-0.24786831233906328, -0.37884183444395647, -0.3581070671527153, -0.2789755784712483, -0.08479506332795332, -0.20506114233561148, -0.0010852099704944287,
                            0.4238786634238386,
                            0.3538826769370612,
                            0.658522319607397
                        ],
                        [-0.2386531663960104,
                            0.35396812553925316, -0.5096729168156166, -0.1010910277174483, -0.1789932295564152, -0.15182077716020137,
                            0.25062331798630016,
                            0.227779868581985, -0.29461070224482205,
                            0.12911703716380712
                        ],
                        [0.17137802065642835,
                            0.08157678240509116, -0.09298895077703563, -0.2628408989738799, -0.4085790950639851, -0.5160163073785606, -0.1265422545249111,
                            0.3779590725878531, -0.1846732913593282,
                            0.4103054680620023
                        ],
                        [0.10274041330701152, -0.12331777697259685,
                            0.3624850868214905, -0.02422794071033323, -0.499754231438732, -0.2326078763311384, -0.045664112769943366,
                            0.0005386706719722756, -0.06859404047647541, -0.6824242063945997
                        ],
                        [-0.3044014688048265,
                            0.044112185945771314,
                            0.15207277764807609,
                            0.604221891849348, -0.005740227943873254,
                            0.11739330139735388, -0.2662113578790332,
                            0.054162694123955836, -0.1758638011283301,
                            0.22252339640652616
                        ],
                        [-0.09869592145969949, -0.27352530937415254,
                            0.48978424883631233,
                            0.33835413613835913, -0.38289423572970976, -0.4343411815910688,
                            0.4197221603308053, -0.32338268741987364,
                            0.11199912031319233, -0.2604923248848677
                        ],
                        [0.6294708697855617,
                            0.0445251605549205, -0.496030618088207, -0.5801466456328007, -0.3395129599314435, -0.3234664857532911,
                            0.16750423944339854, -0.7357802799842111, -0.4172057004030097, -0.03762938641814439
                        ],
                        [-0.17281817719455378, -0.436221988351252, -0.13933266763666988, -0.21417661008552724,
                            0.2734049747740402,
                            0.42455841542748907,
                            0.895376555748988,
                            0.2089149145268151,
                            0.2982801282585924, -0.34337220020344006
                        ],
                        [0.23342130283380924,
                            0.5361982224245471,
                            0.10993113894872833,
                            0.058696296016159666,
                            0.3316860977826941, -0.15012197097664925, -0.3686289311607766, -0.1748925982165964, -0.08878637791136315,
                            0.46314363134558284
                        ]
                    ],
                    [
                        [1.06348228511347],
                        [-0.11327202867606384],
                        [-1.031739213375915],
                        [1.2694728565047546],
                        [-0.6703697192549465],
                        [0.4745247124688098],
                        [1.3147236161446545],
                        [-0.24673802474345344],
                        [1.2691743159560047],
                        [0.2617534868593826],
                        [-0.1547318773291093],
                        [-0.5561832711897426],
                        [1.617617947665187],
                        [-1.3592012028512779],
                        [-0.10245601992474457],
                        [0.48475459884445327],
                        [-0.0922594824651141],
                        [-0.3512996118795069],
                        [-0.4805655821797293],
                        [0.6553924294867092],
                        [-1.3030067231329585],
                        [-0.05763970731179326],
                        [-1.3027530082305838],
                        [-1.902305796401518],
                        [0.8881386397991767],
                        [1.7347206375856816],
                        [-1.0431479894941613],
                        [1.0030086773973337],
                        [0.3678379994761921],
                        [-1.2108344932438393]
                    ],
                    [
                        [0.35908364396985953,
                            0.17609093440787676, -0.1387282975797684,
                            0.3624130632933994,
                            0.11088595165251515, -0.025297747430310127,
                            0.2568969958795377, -0.07664349304623214, -0.11032889594155149, -0.002840083490391532,
                            0.09037256630661077,
                            0.08874805481716105,
                            0.08129763113199255,
                            0.05744922229678959,
                            0.2726179343111334,
                            0.01508016568390385, -0.09447416998799946, -0.06108316906046407,
                            0.18472672599369078,
                            0.15589634157109278,
                            0.031133932791967474,
                            0.09056872602651166,
                            0.14365992372154812, -0.2469322332437878,
                            0.4431100869356929,
                            0.43740416466679954,
                            0.018910779248729265,
                            0.12571714202593426, -0.16520059546708257,
                            0.1660779111161738
                        ],
                        [-0.10915618005802531,
                            0.0013191448771558566,
                            0.0974039486323895, -0.5928962618934296, -0.11362826872900066, -0.25969287594383367, -0.11076426266174044, -0.1610919912762249, -0.1393217249832279, -0.1645577327206121,
                            0.08838440884973864,
                            0.22039554399464106, -0.1054372552103032,
                            0.024509201050635664, -0.026887133247703934, -0.328596844753325, -0.2111370451633122, -0.2392423898683764, -0.09949192652709433, -0.06338452646805395, -0.25147011024475113, -0.16999624481139403, -0.19638837916422688,
                            0.17465462435814313, -0.4351567091467434, -0.20012391583438732,
                            0.04446295652069178, -0.15466253148128517, -0.020892094404367182, -0.12910888677992924
                        ],
                        [0.22940889063650324, -0.023110011984972915, -0.1650992202388904,
                            0.18407623693003516,
                            0.09538244516087707, -0.08623439351850573,
                            0.2667456438942805, -0.024398270728176994, -0.09541091336449652, -0.35014903123456886,
                            0.2201429582506896, -0.11815701928877223, -0.05585617193486395,
                            0.02143012946888866,
                            0.09563117240754386, -0.4427725163302287,
                            0.16840350683933616, -0.08885536752399253,
                            0.00703885330192659, -0.20301523477101224,
                            0.053445147464552534, -0.2648014259912377, -0.19036472918172317, -0.15882748327167823,
                            0.0643257551646384, -0.0721758970930173, -0.04695777258126396,
                            0.11170935243747887,
                            0.033834822021532894, -0.02680323457997105
                        ]
                    ],
                    [
                        [0.38906917498260357],
                        [-0.17306228936431656],
                        [0.21197438957071563]
                    ]
                ])

                //The Matricies above are static


                //Put this part in the 2() functions
                var n = JSON.parse(getElementsByName("numb")[0].value);

                function sigmoid(z) {
                    return 1 / (1 + math.pow(math.e, (-1) * z))
                }

                function forward(a) {
                    z = zipped;
                    console.log(z)
                    for (i = 0; i < 2; i++) {
                        a = sigmoid(math.dot(z[i][0], a) + z[i][1])
                    };
                    return a;
                }

                function pn1() {
                    var input1 = document.getElementsByName("inMatrix1")[0].value;
                    var inputMatrix1 = math.matrix(JSON.parse(input1));
                    console.log(inputMatrix1);
                    console.log(weights);
                    console.log(bias);
                    console.log(forward(inputMatrix1));
                    document.getElementsByName("outputBox1")[0].value = String(inputMatrix1);
                }
            </script>
            -->
            <article id="code">
                <span class="image main"><img src="images/code.jpg" alt=""></span>
                <h2 class="major">The Code</h2>
                <p>The NeurAlgae.py program is the heart and soul of the project. The majority of the code is a class defining our neural network. It is meant to be imported into the python interpreter as a library, along with its complementary file Data.py.
                    You can view the complete NeurAlgae.py code, as well as an explanation of what it does, here on this page, or visit the NeurAlgae GitHub page for all the files used in the project.</p>
                <a href="#allCode" class="button special">View complete code</a>
                <a href="https://github.com/FlowBoat/Flow-Tech-NeurAlgae" class="button special">View project on GitHub</a>
                <p>The first thing the NeurAlgae program does is to declare a class called 'NeuralNet', which defines a neural network. The __init__ function takes in a list of the layer sizes in the network, 'sizes', and creates variables accordingly.</p>
                <pre><code>class NeuralNet(object):
    def __init__(self, sizes):
        #Initialize everything
        self.nLayers = len(sizes)
        self.sizes = sizes
        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.trainCost, self.testCost = [], []
        self.trainDelta, self.testDelta = [], []</code></pre>
                <p>The 'nLayers' variable is simply the number of layers in the network, and the 'sizes' variable is the size list mentioned earlier. The 'weights' variable is a list of arrays of random floating-point numbers which correspond to the sizes
                    of the network - each neuron in one layer is connected by a weight to the neuron in the following layer, so each list element is an array with dimensions equal to the number of neurons in the layers forward and behind. The 'biases'
                    variable is similar to the weights variable, but instead of each neuron having weights for a whole layer, it only has its own bias, so for a layer there is an array with one bias for each junction of weights (meaning no input biases).
                    Finally, the 'trainCost', 'testCost', 'trainDelta', and 'testDelta' variables are lists for storing data about the training of the network.</p>
                <p>The next few functions in the NeuralNet class deal with the propagation of values through the network.</p>
                <pre><code>def sigmoid(self, z):
        #Sigmoid neuron activation function
        return 1 / (1 + np.exp(-z))
    def sigmoidPrime(self, z):
        #Derivative of sigmoid function
        return self.sigmoid(z) * (1 - self.sigmoid(z))
    def forward(self, a):
        #Move values forward through network
        for w, b in zip(self.weights, self.biases):
            a = self.sigmoid(np.dot(w, a) + b)
        return a</pre></code>
                <p>The sigmoid function defines the neural network's activation function as a logistic sigmoid function. This function's graph has an S-like shape, and limits at positive and negative infinity, so for any input the output is a real number
                    between 0 and 1. The sigmoidPrime function is simply the derivative of the sigmoid function (for those unfamiliar with calculus, this function's value at x is the rate of change of the sigmoid function at that same x-value). And the
                    forward function propagates the network's inputs through to the outputs: it calculates the output of a neuron by summing the product of its inputs and their corresponding weights, adding the neuron's bias, and applying the sigmoid
                    function to the result. These simple functions are all it takes to define a neural network (although training it is another matter).</p>
                <p>The next functions deal with assigning 'costs' to the outputs of the network, which are minimized to train the network:</p>
                <pre><code>def costfn(self, a, y):
        #Implement cross-entropy cost function
        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))
    def deltaCost(self, z, a, y):
        #Simple difference between activation and expected output
        return (a - y)
    def totalCost(self, data, lmda):
        #Sum the cost of passing a dataset forward
        cost = 0.0
        for x, y in data:
            a = self.forward(x)
            cost += self.costfn(a, y) / len(data)
        cost += 0.5 * (lmda / len(data)) * sum(np.linalg.norm(w) ** 2 for w in self.weights)
        return cost
    def totalDelta(self, data):
        #Sum the absolute error of passing a dataset forward
        delta = 0.0
        for x, y in data:
            a = self.forward(x)
            delta += (a - y)
        delta = delta / len(data)
        return delta</pre></code>
                <p>The cross-entropy of a and y is used as the cost function. The benefit of this oven another cost function is the property that the rate of change of the cross-entropy cost with respect to the weights (or rather, the learning rate of the
                    network) is proportional to the error in the output, a - y, but not proportional to the rate of change of the sigmoid activation function, which approaches zero. The costfn function implements the cross-entropy cost, whereas the deltaCost
                    function simply returns the difference between the activation and expected outputs. The totalCost and totalDelta functions are used for calculating the costs and errors of passing an entire dataset through the network. The totalCost
                    function also implements L2 regularization by adding the product of the parameter and the size of all the weights squared to the sum of all the costs.</p>
                <p>The backprop function is used to compute the gradient of the cost function to use in the gradent descent optimization:</p>
                <pre><code>def backprop(self, x, y):
        #Compute gradient of the cost function
        delw = [np.zeros(w.shape) for w in self.weights]
        delb = [np.zeros(b.shape) for b in self.biases]
        activation = x
        activations = [x]
        zs = []
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, activation) + b
            zs.append(z)
            activation = self.sigmoid(z)
            activations.append(activation)
        delta = self.deltaCost(zs[-1], activations[-1], y)
        delw[-1] = np.dot(delta, np.transpose(activations[-2]))
        delb[-1] = delta
        for i in range(2, self.nLayers):
            z = zs[-i]
            sp = self.sigmoidPrime(z)
            delta = np.dot(np.transpose(self.weights[-i + 1]), delta) * sp
            delw[-i] = np.dot(delta, np.transpose(activations[-i - 1]))
            delb[-i] = delta
        return (delw, delb)</pre></code>
                <p>The del operator in the code represents the gradient as a vector of what it operates on (the sum of the partial derivatives). Delta refers to the difference. This code passes an x-value forward through the network, saving the inputs and
                    outputs of each neuron as it goes. It then uses these values to calculate gradient vectors for the weights and biases, and then propagates these backwards through the network, assigning each weight and bias with an associated gradient
                    vector, and returns the list of these vectors.</p>
                <p>updateMinBat works by backpropagating a smaller set of data than the full dataset (a mini-batch), and using the resulting gradient vectors to change the weights and biases.</p>
                <pre><code>def updateMinBat(self, minBat, learnRate, lmda, n):
        #Do gradient descent using backprop to a single mini-batch
        delw = [np.zeros(w.shape) for w in self.weights]
        delb = [np.zeros(b.shape) for b in self.biases]
        for x, y in minBat:
            deltadelw, deltadelb = self.backprop(x, y)
            delw = [dw + ddw for dw, ddw in zip(delw, deltadelw)]
            delb = [db + ddb for db, ddb in zip(delb, deltadelb)]
        self.weights = [(1 - learnRate * (lmda / n)) * w - (learnRate / len(minBat)) * dw for w, dw in zip(self.weights, delw)] 
        self.biases = [b - (learnRate / len(minBat)) * db for b, db in zip(self.biases, delb)]</pre></code>
                <p>For a single mini-batch, each data point is backpropagated, and the resulting gradient vectors summed into two larger gradient vectors delw and delb. The weights of the network are then updated by subtracting the gradient vectors, scaled
                    by the learning rate parameter and the size of the minibatch</p>
                <p>The stochastic gradient descent implementation is the basis of training the neural network. It uses gradient descent to minimize the value of the cost function, which results in training being run as an optimization problem.</p>
                <pre><code>def remindSGD(self):
        #Remind forgetful programmers how to train network
        print ("<network>.stochGradDescent(self, train, epochs, minBatSize, learnRate, lmda = 0.0, test = None, monitorTrain = False, monitorTest = False, monitorDelta = False, delay = 0.0)")
    def stochGradDescent(self, train, epochs, minBatSize, learnRate, lmda = 0.0, test = None, monitorTrain = False, monitorTest = False, monitorDelta = False, delay = 0.0):
        #Apply gradient descent
        n = len(train)
        if test: nTest = len(test)
        trainCost, testCost = [], []
        trainDelta, testDelta = [], []
        train2 = train
        if len(self.trainCost) == 0:
            if monitorTrain:
                cost = self.totalCost(train2, lmda)
                trainCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(train2)
                    trainDelta.append(delta)
                    print("Initial error with training data: {}".format(delta))
                print("Initial cost with training data: {}".format(cost))
        if len(self.testCost) == 0:
            if monitorTest:
                cost = self.totalCost(test, lmda)
                testCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(test)
                    testDelta.append(delta)
                    print("Initial error with testing data: {}".format(delta))
                print("Initial cost with testing data: {}".format(cost))
        for i in range(epochs):
            random.shuffle(train2)
            minBats = [train2[j:j + minBatSize] for j in range(0, n, minBatSize)]
            for minBat in minBats:
                self.updateMinBat(minBat, learnRate, lmda, len(train2))
            print("Epoch %s: training complete" % i)
            if monitorTrain:
                cost = self.totalCost(train2, lmda)
                trainCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(train2)
                    trainDelta.append(delta)
                    print("Error with training data: {}".format(delta))
                print("Cost with training data: {}".format(cost))
            if monitorTest:
                cost = self.totalCost(test, lmda)
                testCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(test)
                    testDelta.append(delta)
                    print("Error with testing data: {}".format(delta))
                print("Cost with testing data: {}".format(cost))
            print
            time.sleep(delay)
        self.trainCost += trainCost
        self.testCost += testCost
        self.trainDelta += trainDelta
        self.testDelta += testDelta
        return trainCost, testCost, trainDelta, testDelta</pre></code>
                <p>While the stochGradDescent function seems long and complicated, it is really only a means of organizing the other functions that do the optimization work. To apply gradient descent, the function merely shuffles the training data and generates
                    mini-batches out of it, then passes these mini-batches and the other parameters given to the functions described above. All the rest of the code is simply processes to keep track of the cost and delta values of the network as it is
                    trained. There is also a helper method, remindSGD, which reminds programmers of all the comlex parameters to keep track of while training the network.</p>
                <p>Finally, each network is saveable and loadable by writing to/reading from JSON files containing dumps of all the neural network parameters.</p>
                <pre><code>def save(self, filename):
        #Save a neural network to a file
        data = {"sizes": self.sizes,
                "weights": [w.tolist() for w in self.weights],
                "biases": [b.tolist() for b in self.biases],
                "trainCost": self.trainCost,
                "testCost": self.testCost,
                "trainDelta": [trd.tolist() for trd in self.trainDelta],
                "testDelta": [ted.tolist() for ted in self.testDelta]}
        f = open(filename, "w")
        json.dump(data, f)
        f.close()
def load(filename):
    #Load a previously created neural network from a file
    f = open(filename, "r")
    data = json.load(f)
    f.close()
    net = NeuralNet(data["sizes"])
    net.weights = [np.array(w) for w in data["weights"]]
    net.biases = [np.array(b) for b in data["biases"]]
    net.trainCost = data["trainCost"]
    net.testCost = data["testCost"]
    net.trainDelta = [np.array(trd) for trd in data["trainDelta"]]
    net.testDelta = [np.array(ted) for ted in data["testDelta"]]
    return net</pre></code>
                <p>Data is saved into a dictionary, formatted into lists, and dumped directly into JSON files. The networks are loaded in the opposite manner, by defining a new NeuralNet object from parameters read from a saved dictionary.</p>
                <hr>
                <h3 id="allCode">Complete Code</h3>
                <pre><code># FlowTech | NeurAlgae
## 2017 WWSEF Science Fair | HAB Prediction Using Machine Learning Algorithms

#Describes and trains a neural network for the analysis and prediction of algal bloom data
#Copyright (C) 2017 AH Zachary Trefler and Atif Mahmud

#This program is free software: you can redistribute it and/or modify
#it under the terms of the GNU General Public License as published by
#the Free Software Foundation, either version 3 of the License, or
#(at your option) any later version.

#This program is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU General Public License for more details.

#You should have received a copy of the GNU General Public License
#along with this program.  If not, see <http://www.gnu.org/licenses/>.

#If you have comments, questions, concerns, or you just want to say 'hi',
#email Zachary Trefler at zmct99@gmail.com or Atif Mahmud at atifmahmud101@gmail.com

import json
import random
import sys
import time
import numpy as np
#For a network with inputs <a> through <c>, outputs <a> through <b>, and <n> data points
#Input data in the form [(np.array([[xa1], [xb1], [xc1]]), np.array([[ya1], [yb1]])), (np.array([[xa2], [xb2], [xc2]]), np.array([[ya2], [yb2]])), ... (np.array([[xan], [xbn], [xcn]]), np.array([[yan], [ybn]]))]
class NeuralNet(object):
    def __init__(self, sizes):
        #Initialize everything
        self.nLayers = len(sizes)
        self.sizes = sizes
        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.trainCost, self.testCost = [], []
        self.trainDelta, self.testDelta = [], []
    def sigmoid(self, z):
        #Sigmoid neuron activation function
        return 1 / (1 + np.exp(-z))
    def sigmoidPrime(self, z):
        #Derivative of sigmoid function
        return self.sigmoid(z) * (1 - self.sigmoid(z))
    def forward(self, a):
        #Move values forward through network
        for w, b in zip(self.weights, self.biases):
            a = self.sigmoid(np.dot(w, a) + b)
        return a
    def costfn(self, a, y):
        #Implement cross-entropy cost function
        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))
    def deltaCost(self, z, a, y):
        #Simple difference between activation and expected output
        return (a - y)
    def totalCost(self, data, lmda):
        #Sum the cost of passing a dataset forward
        cost = 0.0
        for x, y in data:
            a = self.forward(x)
            cost += self.costfn(a, y) / len(data)
        cost += 0.5 * (lmda / len(data)) * sum(np.linalg.norm(w) ** 2 for w in self.weights)
        return cost
    def totalDelta(self, data):
        #Sum the absolute error of passing a dataset forward
        delta = 0.0
        for x, y in data:
            a = self.forward(x)
            delta += (a - y)
        delta = delta / len(data)
        return delta
    def backprop(self, x, y):
        #Compute gradient of the cost function
        delw = [np.zeros(w.shape) for w in self.weights]
        delb = [np.zeros(b.shape) for b in self.biases]
        activation = x
        activations = [x]
        zs = []
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, activation) + b
            zs.append(z)
            activation = self.sigmoid(z)
            activations.append(activation)
        delta = self.deltaCost(zs[-1], activations[-1], y)
        delw[-1] = np.dot(delta, np.transpose(activations[-2]))
        delb[-1] = delta
        for i in range(2, self.nLayers):
            z = zs[-i]
            sp = self.sigmoidPrime(z)
            delta = np.dot(np.transpose(self.weights[-i + 1]), delta) * sp
            delw[-i] = np.dot(delta, np.transpose(activations[-i - 1]))
            delb[-i] = delta
        return (delw, delb)
    def updateMinBat(self, minBat, learnRate, lmda, n):
        #Do gradient descent using backprop to a single mini-batch
        delw = [np.zeros(w.shape) for w in self.weights]
        delb = [np.zeros(b.shape) for b in self.biases]
        for x, y in minBat:
            deltadelw, deltadelb = self.backprop(x, y)
            delw = [dw + ddw for dw, ddw in zip(delw, deltadelw)]
            delb = [db + ddb for db, ddb in zip(delb, deltadelb)]
        self.weights = [(1 - learnRate * (lmda / n)) * w - (learnRate / len(minBat)) * dw for w, dw in zip(self.weights, delw)] 
        self.biases = [b - (learnRate / len(minBat)) * db for b, db in zip(self.biases, delb)]
    def remindSGD(self):
        #Remind forgetful programmers how to train network
        print ("<network>.stochGradDescent(self, train, epochs, minBatSize, learnRate, lmda = 0.0, test = None, monitorTrain = False, monitorTest = False, monitorDelta = False, delay = 0.0)")
    def stochGradDescent(self, train, epochs, minBatSize, learnRate, lmda = 0.0, test = None, monitorTrain = False, monitorTest = False, monitorDelta = False, delay = 0.0):
        #Apply gradient descent
        n = len(train)
        if test: nTest = len(test)
        trainCost, testCost = [], []
        trainDelta, testDelta = [], []
        train2 = train
        if len(self.trainCost) == 0:
            if monitorTrain:
                cost = self.totalCost(train2, lmda)
                trainCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(train2)
                    trainDelta.append(delta)
                    print("Initial error with training data: {}".format(delta))
                print("Initial cost with training data: {}".format(cost))
        if len(self.testCost) == 0:
            if monitorTest:
                cost = self.totalCost(test, lmda)
                testCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(test)
                    testDelta.append(delta)
                    print("Initial error with testing data: {}".format(delta))
                print("Initial cost with testing data: {}".format(cost))
        for i in range(epochs):
            random.shuffle(train2)
            minBats = [train2[j:j + minBatSize] for j in range(0, n, minBatSize)]
            for minBat in minBats:
                self.updateMinBat(minBat, learnRate, lmda, len(train2))
            print("Epoch %s: training complete" % i)
            if monitorTrain:
                cost = self.totalCost(train2, lmda)
                trainCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(train2)
                    trainDelta.append(delta)
                    print("Error with training data: {}".format(delta))
                print("Cost with training data: {}".format(cost))
            if monitorTest:
                cost = self.totalCost(test, lmda)
                testCost.append(cost)
                if monitorDelta:
                    delta = self.totalDelta(test)
                    testDelta.append(delta)
                    print("Error with testing data: {}".format(delta))
                print("Cost with testing data: {}".format(cost))
            print
            time.sleep(delay)
        self.trainCost += trainCost
        self.testCost += testCost
        self.trainDelta += trainDelta
        self.testDelta += testDelta
        return trainCost, testCost, trainDelta, testDelta
    def save(self, filename):
        #Save a neural network to a file
        data = {"sizes": self.sizes,
                "weights": [w.tolist() for w in self.weights],
                "biases": [b.tolist() for b in self.biases],
                "trainCost": self.trainCost,
                "testCost": self.testCost,
                "trainDelta": [trd.tolist() for trd in self.trainDelta],
                "testDelta": [ted.tolist() for ted in self.testDelta]}
        f = open(filename, "w")
        json.dump(data, f)
        f.close()
def load(filename):
    #Load a previously created neural network from a file
    f = open(filename, "r")
    data = json.load(f)
    f.close()
    net = NeuralNet(data["sizes"])
    net.weights = [np.array(w) for w in data["weights"]]
    net.biases = [np.array(b) for b in data["biases"]]
    net.trainCost = data["trainCost"]
    net.testCost = data["testCost"]
    net.trainDelta = [np.array(trd) for trd in data["trainDelta"]]
    net.testDelta = [np.array(ted) for ted in data["testDelta"]]
    return net</code></pre>
            </article>
            <!-- Elements -->
            <article id="elements">
                <h2 class="major">Elements</h2>

                <section>
                    <h3 class="major">Text</h3>
                    <p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>. This is <sup>superscript</sup> text and this is <sub>subscript</sub> text. This is <u>underlined</u> and this is code:
                        <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
                    <hr />
                    <h2>Heading Level 2</h2>
                    <h3>Heading Level 3</h3>
                    <h4>Heading Level 4</h4>
                    <h5>Heading Level 5</h5>
                    <h6>Heading Level 6</h6>
                    <hr />
                    <h4>Blockquote</h4>
                    <blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum
                        ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
                    <h4>Preformatted</h4>
                    <pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
                </section>

                <section>
                    <h3 class="major">Lists</h3>

                    <h4>Unordered</h4>
                    <ul>
                        <li>Dolor pulvinar etiam.</li>
                        <li>Sagittis adipiscing.</li>
                        <li>Felis enim feugiat.</li>
                    </ul>

                    <h4>Alternate</h4>
                    <ul class="alt">
                        <li>Dolor pulvinar etiam.</li>
                        <li>Sagittis adipiscing.</li>
                        <li>Felis enim feugiat.</li>
                    </ul>

                    <h4>Ordered</h4>
                    <ol>
                        <li>Dolor pulvinar etiam.</li>
                        <li>Etiam vel felis viverra.</li>
                        <li>Felis enim feugiat.</li>
                        <li>Dolor pulvinar etiam.</li>
                        <li>Etiam vel felis lorem.</li>
                        <li>Felis enim et feugiat.</li>
                    </ol>
                    <h4>Icons</h4>
                    <ul class="icons">
                        <li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
                        <li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
                        <li><a href="#" class="icon fa-instagram"><span class="label">Instagram</span></a></li>
                        <li><a href="#" class="icon fa-github"><span class="label">Github</span></a></li>
                    </ul>

                    <h4>Actions</h4>
                    <ul class="actions">
                        <li><a href="#" class="button special">Default</a></li>
                        <li><a href="#" class="button">Default</a></li>
                    </ul>
                    <ul class="actions vertical">
                        <li><a href="#" class="button special">Default</a></li>
                        <li><a href="#" class="button">Default</a></li>
                    </ul>
                </section>

                <section>
                    <h3 class="major">Table</h3>
                    <h4>Default</h4>
                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Name</th>
                                    <th>Description</th>
                                    <th>Price</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Item One</td>
                                    <td>Ante turpis integer aliquet porttitor.</td>
                                    <td>29.99</td>
                                </tr>
                                <tr>
                                    <td>Item Two</td>
                                    <td>Vis ac commodo adipiscing arcu aliquet.</td>
                                    <td>19.99</td>
                                </tr>
                                <tr>
                                    <td>Item Three</td>
                                    <td> Morbi faucibus arcu accumsan lorem.</td>
                                    <td>29.99</td>
                                </tr>
                                <tr>
                                    <td>Item Four</td>
                                    <td>Vitae integer tempus condimentum.</td>
                                    <td>19.99</td>
                                </tr>
                                <tr>
                                    <td>Item Five</td>
                                    <td>Ante turpis integer aliquet porttitor.</td>
                                    <td>29.99</td>
                                </tr>
                            </tbody>
                            <tfoot>
                                <tr>
                                    <td colspan="2"></td>
                                    <td>100.00</td>
                                </tr>
                            </tfoot>
                        </table>
                    </div>

                    <h4>Alternate</h4>
                    <div class="table-wrapper">
                        <table class="alt">
                            <thead>
                                <tr>
                                    <th>Name</th>
                                    <th>Description</th>
                                    <th>Price</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Item One</td>
                                    <td>Ante turpis integer aliquet porttitor.</td>
                                    <td>29.99</td>
                                </tr>
                                <tr>
                                    <td>Item Two</td>
                                    <td>Vis ac commodo adipiscing arcu aliquet.</td>
                                    <td>19.99</td>
                                </tr>
                                <tr>
                                    <td>Item Three</td>
                                    <td> Morbi faucibus arcu accumsan lorem.</td>
                                    <td>29.99</td>
                                </tr>
                                <tr>
                                    <td>Item Four</td>
                                    <td>Vitae integer tempus condimentum.</td>
                                    <td>19.99</td>
                                </tr>
                                <tr>
                                    <td>Item Five</td>
                                    <td>Ante turpis integer aliquet porttitor.</td>
                                    <td>29.99</td>
                                </tr>
                            </tbody>
                            <tfoot>
                                <tr>
                                    <td colspan="2"></td>
                                    <td>100.00</td>
                                </tr>
                            </tfoot>
                        </table>
                    </div>
                </section>

                <section>
                    <h3 class="major">Buttons</h3>
                    <ul class="actions">
                        <li><a href="#" class="button special">Special</a></li>
                        <li><a href="#" class="button">Default</a></li>
                    </ul>
                    <ul class="actions">
                        <li><a href="#" class="button">Default</a></li>
                        <li><a href="#" class="button small">Small</a></li>
                    </ul>
                    <ul class="actions">
                        <li><a href="#" class="button special icon fa-download">Icon</a></li>
                        <li><a href="#" class="button icon fa-download">Icon</a></li>
                    </ul>
                    <ul class="actions">
                        <li><span class="button special disabled">Disabled</span></li>
                        <li><span class="button disabled">Disabled</span></li>
                    </ul>
                </section>

                <section>
                    <h3 class="major">Form</h3>
                    <form method="post" action="#">
                        <div class="field half first">
                            <label for="demo-name">Name</label>
                            <input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
                        </div>
                        <div class="field half">
                            <label for="demo-email">Email</label>
                            <input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
                        </div>
                        <div class="field">
                            <label for="demo-category">Category</label>
                            <div class="select-wrapper">
                                <select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
                            </div>
                        </div>
                        <div class="field half first">
                            <input type="radio" id="demo-priority-low" name="demo-priority" checked>
                            <label for="demo-priority-low">Low</label>
                        </div>
                        <div class="field half">
                            <input type="radio" id="demo-priority-high" name="demo-priority">
                            <label for="demo-priority-high">High</label>
                        </div>
                        <div class="field half first">
                            <input type="checkbox" id="demo-copy" name="demo-copy">
                            <label for="demo-copy">Email me a copy</label>
                        </div>
                        <div class="field half">
                            <input type="checkbox" id="demo-human" name="demo-human" checked>
                            <label for="demo-human">Not a robot</label>
                        </div>
                        <div class="field">
                            <label for="demo-message">Message</label>
                            <textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
                        </div>
                        <ul class="actions">
                            <li><input type="submit" value="Send Message" class="special" /></li>
                            <li><input type="reset" value="Reset" /></li>
                        </ul>
                    </form>
                </section>

            </article>

        </div>

        <!-- Footer -->
        <footer id="footer">
            <p class="copyright">&copy; Atif Mahmud. Design:
                <a href="http://github.com/Atif-Mahmud">Visit.</a>
            </p>
        </footer>

    </div>

    <!-- BG -->
    <div id="bg"></div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>